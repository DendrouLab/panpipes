# ============================================================
# Preprocess workflow Panpipes (pipeline_preprocess.py)
# ============================================================
# This file contains the parameters for the ingest workflow.
# For full descriptions of the parameters, see the documentation at https://panpipes-pipelines.readthedocs.io/en/latest/yaml_docs/pipeline_preprocess_yml.html


#--------------------------
# Compute resources options
#--------------------------
resources:
  threads_high: 2
  threads_medium: 2
  threads_low: 1

condaenv:

# allows for tweaking where the jobs get submitted, in case there is a special queue for long jobs or you have access to a gpu
# leave as is if you do not want to use the alternative queues

#-------------------------------
# General project specifications
#-------------------------------
sample_prefix: 
unfiltered_obj: 

modalities:
  rna:  True
  prot: False
  rep: False
  atac: False

# ----------------------------
# Filtering Cells and Features
# ----------------------------
# Filtering is done sequentially for all modalities, filtering first cells and then features.
# In the following, you can specify the filtering parameters for each modality.

filtering:
  run: True
  keep_barcodes:

  #------------------------
  # RNA-specific filtering
  rna:
    # obs, i.e. cell level filtering
    obs:
      min:
        n_genes_by_counts: 
      max:
        total_counts: 
        n_genes_by_counts: 
        # percent filtering: 
        # this should be a value between 0 and 100%. 
        # leave blank or set to 100 to avoid filtering for any of these param
        pct_counts_mt: 
        pct_counts_rp:
        # either one score for all samples e.g. 0.25, 
        # or a csv file with two columns sample_id, and cut off
        # if you want to apply a custom scrublet threshold per input sample you can specify it as a max value here
        doublet_scores:
      # you could add a new column to the mudata['rna'].obs with True False values, and list
      # that column under bool:, you can do this for any modality
      bool:
    ## var filtering: (feature) gene level filtering here
    var:
      min:
        n_cells_by_counts: 
      max:
        total_counts:
        n_cells_by_counts:
  #------------------------------------------------------
  prot:
  #------------------------------------------------------
    ## obs filtering: cell level filtering here
    obs:  
      max:
        total_counts:
    ##var filtering: (feature) protein level filtering here
    var:
      max:
      min:
  #------------------------------------------------------
  atac:
  #------------------------------------------------------
    ## obs filtering: cell level filtering here
    obs:  
      max:
    ## var filtering: (feature) fragment level filtering here
    var:    
      nucleosome_signal:

# ----------------------
# Intersecting cell barcodes
# ----------------------
# Subset observations (cells) in-place by intersect
# taking observations present only in modalities listed in mods, or all modalities if mods is None.  
# set a comma separated list where you want to keep only the intersection of barcodes. e.g. rna,prot 
intersect_mods: 

# ----------------------
# Downsampling cell barcodes
# ----------------------
# how many cells to downsample to, leave blank to keep all cells.
downsample_n: 
# if we want to equalise by dataset or sample_id then specifiy a column in obs
# then the data will be subset to n cells **per** downsample_col value.
downsample_col: 
#  which modalities do we want to subsample
# comma separated string e.g. rna,prot
# if more than 1 modality is added then these will be intersected.
downsample_mods: 

# ----------------------
# Plotting variables
# ----------------------
# all metrics should be inputted as a comma separated string without spaces e.g. a,b,c
# leave blank to avoid plotting
plotqc:
  # use these categorical variables to plot/split by
  grouping_var: sample_id
  # specify metrics in the metadata to plot:
  rna_metrics: pct_counts_mt,pct_counts_rp,pct_counts_hb,pct_counts_ig,doublet_scores
  prot_metrics: total_counts,log1p_total_counts,n_prot_by_counts,pct_counts_isotype
  atac_metrics: 
  rep_metrics:
# --------------------------------------------------------------------------------------------------------
# RNA steps
# --------------------------------------------------------------------------------------------------------
# currently only standard sc.pp.normalize_total(adata, target_sum=1e4) followed by sc.pp.log1p(adata) is offered for RNA
log1p: True
# hvg_flavour options include "seurat", "cell_ranger", "seurat_v3", default; "seurat"
# for dispersion based methods "seurat" and "cell_ranger", you can specify parameters: min_mean, max_mean, min_disp
# for "seurat_v3" a different method is used, and you need to specify how many variable genes to find in n_top_genes
# If you specify n_top_genes, then the other paramters are nulled.
# details: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.highly_variable_genes.html
hvg:
  flavor: seurat # "seurat", "cell_ranger", "seurat_v3"
  # If batch key is specified, highly-variable genes are selected within each batch separately and merged. 
  # details: https://scanpy.readthedocs.io/en/stable/generated/scanpy.pp.highly_variable_genes.html#:~:text=or%20return%20them.-,batch_key,-%3A%20Optional%5B
  # If you want to use more than one obs column as a covariates, include it as covariate1,covariate2 (comma separated list)
  # Leave blank for no batch (default)
  batch_key:
  n_top_genes: 2000
  min_mean:
  max_mean:
  min_disp:
  # It may be useful to exclude some genes from the HVG selection. 
  # In the file resources/qc_genelist_1.0.csv, we include an example of genes that could be excluded when analysing immune cells, 
  # Examine this file, it has a first column with gene ids and the second column identifying the groups to
  # which this genes belong. 
  # This workflow will exclude the genes that you specify by their group name. when specifying "default", the workflows will
  # remove from hvg the genes that in the file are flagged "exclude". You can customize the gene list and change the name of the gene group in
  # the `exclude:` param accordingly.
  exclude_file: 
  exclude: # this is the variable that defines the genes to be excluded in the above file, leave empty if you're not excluding genes from HVG
  # Do you want to filter the object to retain only Highly Variable Genes?
  filter: False
# Regression variables, what do you want to regress out, leave blank if nothing
# We recommend not regressing out unless you have good reason to.
regress_variables:
#----------------------------
# Scaling
#----------------------------
# This scaling has the effect that all genes are weighted equally for downstream analysis. 
# discussion from Leucken et al Best Practices paper: https://doi.org/10.15252/msb.20188746
# "There is currently no consensus on whether or not to perform normalization over genes. 
# While the popular Seurat tutorials (Butler et al, 2018) generally apply gene scaling, 
# the authors of the Slingshot method opt against scaling over genes in their tutorial (Street et al, 2018). 
# The preference between the two choices revolves around whether all genes should be weighted equally for downstream analysis, 
# or whether the magnitude of expression of a gene is an informative proxy for the importance of the gene."
run_scale: True
# if blank defaults scale, clip values as per: https://scanpy.readthedocs.io/en/stable/api/scanpy.pp.scale.html
scale_max_value: 

#----------------------------
# RNA Dimensionality reduction
#----------------------------
pca:
  # number of components to compute
  n_pcs: 50
  # set "default" will use 'arpack'. 
  # Otherwise specify a different solver, see https://scanpy.readthedocs.io/en/stable/generated/scanpy.tl.pca.html
  solver: default 
  # color to use when plotting the PCA
  color_by: sample_id

# --------------------------------------------------------------------------------------------------------
# Protein (PROT) steps
# --------------------------------------------------------------------------------------------------------
prot:
  # comma separated string of normalisation options
  # options: dsb,clr 
  # more details in this vignette https://muon.readthedocs.io/en/latest/omics/citeseq.html
  # dsb https://muon.readthedocs.io/en/latest/api/generated/muon.prot.pp.dsb.html
  # clr https://muon.readthedocs.io/en/latest/api/generated/muon.prot.pp.clr.html
  normalisation_methods: clr,dsb
  # the normalised matrices are stored in layers called 'clr' and 'dsb', along with a layer called 'raw_counts' 
  # if you choose to run both then 'dsb' is stored in X as default.
  # In downstream visualisation, you can either specify the layer, or take the default.

  # CLR parameters:
  # margin determines whether you normalise per cell (as you would RNA norm), 
  # or by feature (recommended, due to the variable nature of prot assays). 
  # CLR margin 0 is recommended for informative qc plots in this pipeline
  # 0 = normalise colwise (per feature)
  # 1 = normalise rowwise (per cell)
  clr_margin: 0

  # DSB parameters:
  # you must specify the path to the background h5mu created in pipeline ingest in order to run dsb.
  background_obj:
  # quantile clipping, 
  # even with normalisation, some cells get extreme outliers which can be clipped as discussed https://github.com/niaid/dsb
  # maximum value will be set at the value of the 99.5% quantile, applied per feature
  # note that this feature is in the default muon mu.pp.dsb code, but manually implemented in this code.
  quantile_clipping: True
  
  # which normalisation method to be stored in the X slot. If you choose to run more than one normalisation method,
  # which one to you want to store in the X slot, if not specified 'dsb' is the default when run.
  store_as_X: 

  # do you want to save the prot normalised assay additionally as a txt file:
  save_norm_prot_mtx: False
  #----------------------------
  # Prot Dimensionality reduction
  #----------------------------
  # it may be useful to run PCA on the protein assay, when you have more than 50 features.
  # Set to False by default
  pca: False
  # number of components. Specify at least n_pcs <= number of features -1 
  n_pcs: 50
  # which solver to use, set "default" will use 'arpack'. 
  solver: default
  # column to be fetched from the protein layer .obs
  color_by: sample_id

# --------------------------------------------------------------------------------------------------------
# ATAC steps
# --------------------------------------------------------------------------------------------------------
atac:
  binarize: False
  normalize: TFIDF # "log1p" or "TFIDF"
  # if normalize = "TFIDF", else leave blank:
  TFIDF_flavour: signac # "signac", "logTF" or "logIDF"
  # highly variable feature selection:
  # HVF selection either with scanpy's pp.highly_variable_genes() function or a pseudo-FindTopFeatures() function of the signac package
  feature_selection_flavour: signac  # "signac" or "scanpy"
  # parameters for HVF flavour == "scanpy", leave the below blank to use defaults
  min_mean: #default 0.05
  max_mean: #default 1.5
  min_disp: #default 0.5
  # if n_top_features is specified, it overwrites previous defaults for HVF selection
  n_top_features:
  # Filter the atac layer to retain only HVF 
  filter_by_hvf: False
  # parameter for HVF flavour == "signac"
  min_cutoff: q5
  # min_cutoff can be specified as follows:
  #   "q[x]": "q" followed by the minimum percentile, e.g. q5 will set the top 95% most common features as higly variable
  #   "c[x]": "c" followed by a minimum cell count, e.g. c100 will set features present in > 100 cells as highly variable
  #   "tc[x]": "tc" followed by a minimum total count, e.g. tc100 will set features with total counts > 100 as highly variable
  #   "NULL": All features are assigned as highly variable
  #   "NA": Highly variable features won't be changed
  #----------------------------
  # ATAC Dimensionality reduction
  #----------------------------
  dimred: LSI #PCA or LSI (LSI will only be computed if the normalize param is set to TFIDF)
  n_comps: 50 # how many components to compute
  # which dimension to exclude from further processing (sometimes useful to remove PC/LSI_1  if it's associated to tech factors)
  # leave blank to retain all 
  # if using PCA, which solver to use. Default == 'arpack'
  solver: default
  # what covariate to use to color the dimensionality reduction
  color_by: sample_id
  # whether to remove the component(s) associated to technical effects, common to remove 1 for LSI
  # leave blank to avoid removing any
  dim_remove: 


